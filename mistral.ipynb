{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanna text to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vanna[chromadb,ollama,postgres] in ./venv/lib/python3.9/site-packages (0.7.9)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (2.32.3)\n",
      "Requirement already satisfied: tabulate in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (0.9.0)\n",
      "Requirement already satisfied: plotly in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (6.1.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (2.2.3)\n",
      "Requirement already satisfied: sqlparse in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (0.5.3)\n",
      "Requirement already satisfied: kaleido in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (0.2.1)\n",
      "Requirement already satisfied: flask in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (3.1.1)\n",
      "Requirement already satisfied: flask-sock in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (0.7.0)\n",
      "Requirement already satisfied: flasgger in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (0.9.7.1)\n",
      "Requirement already satisfied: sqlalchemy in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (2.0.41)\n",
      "Requirement already satisfied: ollama in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (0.4.8)\n",
      "Requirement already satisfied: httpx in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (0.28.1)\n",
      "Requirement already satisfied: psycopg2-binary in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (2.9.10)\n",
      "Requirement already satisfied: db-dtypes in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (1.4.3)\n",
      "Requirement already satisfied: chromadb<1.0.0 in ./venv/lib/python3.9/site-packages (from vanna[chromadb,ollama,postgres]) (0.6.3)\n",
      "Requirement already satisfied: build>=1.0.3 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2.11.4)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.115.12)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.34.2)\n",
      "Requirement already satisfied: numpy>=1.22.5 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2.0.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (4.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (4.13.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.33.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.33.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.54b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.33.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.15.4)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (9.1.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (3.10.18)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./venv/lib/python3.9/site-packages (from chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (14.0.0)\n",
      "Requirement already satisfied: packaging>=19.1 in ./venv/lib/python3.9/site-packages (from build>=1.0.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in ./venv/lib/python3.9/site-packages (from build>=1.0.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in ./venv/lib/python3.9/site-packages (from build>=1.0.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (8.6.1)\n",
      "Requirement already satisfied: tomli>=1.1.0 in ./venv/lib/python3.9/site-packages (from build>=1.0.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2.2.1)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in ./venv/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.46.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.9/site-packages (from pydantic>=1.9->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.9/site-packages (from pydantic>=1.9->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic>=1.9->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in ./venv/lib/python3.9/site-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.3.1)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.9/site-packages (from httpx->vanna[chromadb,ollama,postgres]) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.9/site-packages (from httpx->vanna[chromadb,ollama,postgres]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.9/site-packages (from httpcore==1.*->httpx->vanna[chromadb,ollama,postgres]) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.9/site-packages (from importlib-metadata>=4.6->build>=1.0.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (3.21.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2.40.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2.4.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./venv/lib/python3.9/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in ./venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (25.2.10)\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (5.29.4)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.14.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./venv/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.2.18)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./venv/lib/python3.9/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.17.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./venv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.33.0 in ./venv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.33.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.33.0 in ./venv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.33.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b0 in ./venv/lib/python3.9/site-packages (from opentelemetry-sdk>=1.2.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.54b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.54b0 in ./venv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.54b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.54b0 in ./venv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.54b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.54b0 in ./venv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.54b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in ./venv/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in ./venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests->vanna[chromadb,ollama,postgres]) (3.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.9/site-packages (from rich>=10.11.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.9/site-packages (from rich>=10.11.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./venv/lib/python3.9/site-packages (from tokenizers>=0.13.2->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.31.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (2025.3.2)\n",
      "Requirement already satisfied: click<8.2,>=8.0.0 in ./venv/lib/python3.9/site-packages (from typer>=0.9.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.9/site-packages (from typer>=0.9.0->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in ./venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (10.0)\n",
      "Requirement already satisfied: pyarrow>=13.0.0 in ./venv/lib/python3.9/site-packages (from db-dtypes->vanna[chromadb,ollama,postgres]) (20.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.9/site-packages (from pandas->vanna[chromadb,ollama,postgres]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.9/site-packages (from pandas->vanna[chromadb,ollama,postgres]) (2025.2)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in ./venv/lib/python3.9/site-packages (from flasgger->vanna[chromadb,ollama,postgres]) (4.23.0)\n",
      "Requirement already satisfied: mistune in ./venv/lib/python3.9/site-packages (from flasgger->vanna[chromadb,ollama,postgres]) (3.1.3)\n",
      "Requirement already satisfied: blinker>=1.9.0 in ./venv/lib/python3.9/site-packages (from flask->vanna[chromadb,ollama,postgres]) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in ./venv/lib/python3.9/site-packages (from flask->vanna[chromadb,ollama,postgres]) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in ./venv/lib/python3.9/site-packages (from flask->vanna[chromadb,ollama,postgres]) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in ./venv/lib/python3.9/site-packages (from flask->vanna[chromadb,ollama,postgres]) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in ./venv/lib/python3.9/site-packages (from flask->vanna[chromadb,ollama,postgres]) (3.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./venv/lib/python3.9/site-packages (from jsonschema>=3.0.1->flasgger->vanna[chromadb,ollama,postgres]) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.9/site-packages (from jsonschema>=3.0.1->flasgger->vanna[chromadb,ollama,postgres]) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.9/site-packages (from jsonschema>=3.0.1->flasgger->vanna[chromadb,ollama,postgres]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.9/site-packages (from jsonschema>=3.0.1->flasgger->vanna[chromadb,ollama,postgres]) (0.25.0)\n",
      "Requirement already satisfied: simple-websocket>=0.5.1 in ./venv/lib/python3.9/site-packages (from flask-sock->vanna[chromadb,ollama,postgres]) (1.1.0)\n",
      "Requirement already satisfied: wsproto in ./venv/lib/python3.9/site-packages (from simple-websocket>=0.5.1->flask-sock->vanna[chromadb,ollama,postgres]) (1.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./venv/lib/python3.9/site-packages (from plotly->vanna[chromadb,ollama,postgres]) (1.39.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb<1.0.0->vanna[chromadb,ollama,postgres]) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'vanna[chromadb,ollama,postgres]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityakumarraj/Desktop/text2sql/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from vanna.ollama import Ollama\n",
    "from vanna.chromadb import ChromaDB_VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVanna(ChromaDB_VectorStore, Ollama):\n",
    "    def __init__(self, config=None):\n",
    "        ChromaDB_VectorStore.__init__(self, config=config)\n",
    "        Ollama.__init__(self, config=config)\n",
    "\n",
    "vn = MyVanna(config={'model': 'mistral', 'allow_llm_to_see_data': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, question, content, training_data_type]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "a = vn.get_training_data()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in ./venv/lib/python3.9/site-packages (2.9.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn.connect_to_postgres(\n",
    "    dbname=\"data\",\n",
    "    user=\"adityakumarraj\",\n",
    "    password=\"Aa%%407909013706\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Structure for public.nodedata:\n",
      "    column_name                    data_type\n",
      "0        nodeid                         text\n",
      "1      nodename                         text\n",
      "2   clustername                         text\n",
      "3  instancetype                         text\n",
      "4          tags                         text\n",
      "5       created  timestamp without time zone\n",
      "6  snapshottime  timestamp without time zone\n",
      "7      platform                         text\n"
     ]
    }
   ],
   "source": [
    "# Query to get detailed column information\n",
    "table_info_sql = \"\"\"\n",
    "SELECT \n",
    "    column_name,\n",
    "    data_type\n",
    "FROM \n",
    "    INFORMATION_SCHEMA.COLUMNS \n",
    "WHERE \n",
    "    table_name = 'nodedata' \n",
    "    AND table_schema = 'public'\n",
    "ORDER BY \n",
    "    ordinal_position;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "table_description = vn.run_sql(table_info_sql)\n",
    "print(\"Table Structure for public.nodedata:\")\n",
    "print(table_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_information_schema = vn.run_sql(\"SELECT * FROM INFORMATION_SCHEMA.COLUMNS where table_name='nodedata' AND table_schema='public';\")\n",
    "df_information_schema\n",
    "# This will break up the information schema into bite-sized chunks that can be referenced by the LLM\n",
    "plan = vn.get_training_plan_generic(df_information_schema)\n",
    "plan\n",
    "# If you like the plan, then uncomment this and run it to train\n",
    "vn.train(plan=plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>content</th>\n",
       "      <th>training_data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54f451ac-e1fd-573e-9efe-b95ac41a1d06-doc</td>\n",
       "      <td>None</td>\n",
       "      <td>The following columns are in the nodedata tabl...</td>\n",
       "      <td>documentation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id question  \\\n",
       "0  54f451ac-e1fd-573e-9efe-b95ac41a1d06-doc     None   \n",
       "\n",
       "                                             content training_data_type  \n",
       "0  The following columns are in the nodedata tabl...      documentation  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = vn.get_training_data()\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vanna.flask import VannaFlaskApp\n",
    "# app = VannaFlaskApp(vn)\n",
    "# app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documentation....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'eb32a74c-4bf1-5e0e-925f-6d738239b1d6-doc'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn.train(documentation='''\n",
    "The table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\n",
    "Note the schema of the table nodedatafor which you will be generating queries:\n",
    "    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\n",
    "    nodename      TEXT,    -- Name of the node\n",
    "    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\n",
    "    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\n",
    "    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\n",
    "    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\n",
    "    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\n",
    "    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\n",
    ")\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documentation....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'b7dca632-4126-522e-9346-a62d6168a75b-doc'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn.train(documentation='''\n",
    "You are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\n",
    "Primary Key:\n",
    "• Ensure nodeid is treated as the primary key—each node is uniquely identified.\n",
    "\n",
    "Date Ranges:\n",
    "• Interpret relative time windows on the created and snapshottime columns:\n",
    "  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\n",
    "  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\n",
    "  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\n",
    "\n",
    "HAVING Clause Rule:\n",
    "• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\n",
    "\n",
    "\n",
    "In queries always refer to the table as 'nodedata' not as 'data.nodedata'.\n",
    "Generate syntactically correct PostgreSQL queries that can be directly executed.\n",
    "Always verify column existence and ensure that queries are efficient and correct.\n",
    "If a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         id question  \\\n",
      "0  54f451ac-e1fd-573e-9efe-b95ac41a1d06-doc     None   \n",
      "1  eb32a74c-4bf1-5e0e-925f-6d738239b1d6-doc     None   \n",
      "2  b7dca632-4126-522e-9346-a62d6168a75b-doc     None   \n",
      "\n",
      "                                             content training_data_type  \n",
      "0  The following columns are in the nodedata tabl...      documentation  \n",
      "1  \\nThe table name is 'nodedata' and it is prese...      documentation  \n",
      "2  \\nYou are an AI assistant trained to generate ...      documentation  \n"
     ]
    }
   ],
   "source": [
    "training_data = vn.get_training_data()\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Vanna Model with Various Questions\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test 1: How many nodes are there per cluster?\n",
      "\n",
      "Generated SQL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Prompt: [{'role': 'system', 'content': \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {'role': 'user', 'content': 'How many nodes are there per cluster?'}]\n",
      "Info: Ollama parameters:\n",
      "model=mistral:latest,\n",
      "options={},\n",
      "keep_alive=None\n",
      "Info: Prompt Content:\n",
      "[{\"role\": \"system\", \"content\": \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {\"role\": \"user\", \"content\": \"How many nodes are there per cluster?\"}]\n",
      "Info: Ollama Response:\n",
      "model='mistral:latest' created_at='2025-05-19T09:50:54.869285Z' done=True done_reason='stop' total_duration=3047853500 load_duration=12842542 prompt_eval_count=1053 prompt_eval_duration=1556542167 eval_count=90 eval_duration=1472643125 message=Message(role='assistant', content=\" To find out how many nodes there are per cluster in the 'nodedata' table, you can use the following SQL query:\\n\\n```sql\\nSELECT clustername, COUNT(nodeid) AS node_count\\nFROM nodedata\\nGROUP BY clustername;\\n```\\n\\nThis query will group all nodes by their respective cluster names and count them. The result will display the number of nodes per cluster.\", images=None, tool_calls=None)\n",
      "LLM Response:  To find out how many nodes there are per cluster in the 'nodedata' table, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT clustername, COUNT(nodeid) AS node_count\n",
      "FROM nodedata\n",
      "GROUP BY clustername;\n",
      "```\n",
      "\n",
      "This query will group all nodes by their respective cluster names and count them. The result will display the number of nodes per cluster.\n",
      "Info: Output from LLM:  To find out how many nodes there are per cluster in the 'nodedata' table, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT clustername, COUNT(nodeid) AS node_count\n",
      "FROM nodedata\n",
      "GROUP BY clustername;\n",
      "```\n",
      "\n",
      "This query will group all nodes by their respective cluster names and count them. The result will display the number of nodes per cluster. \n",
      "Extracted SQL: SELECT clustername, COUNT(nodeid) AS node_count\n",
      "FROM nodedata\n",
      "GROUP BY clustername\n",
      "SELECT clustername, COUNT(nodeid) AS node_count\n",
      "FROM nodedata\n",
      "GROUP BY clustername\n",
      "\n",
      "Query Result:\n",
      "                      clustername  node_count\n",
      "0       sensei-eks04-prod-cluster         420\n",
      "1       sensei-eks01-prod-cluster          11\n",
      "2        colligo-laser01-prod-uw2        2195\n",
      "3  colligo-laser-control-prod-uw2           7\n",
      "4       sensei-eks02-prod-cluster         188\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test 2: Show me the nodes created in the last 7 days\n",
      "\n",
      "Generated SQL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Prompt: [{'role': 'system', 'content': \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {'role': 'user', 'content': 'Show me the nodes created in the last 7 days'}]\n",
      "Info: Ollama parameters:\n",
      "model=mistral:latest,\n",
      "options={},\n",
      "keep_alive=None\n",
      "Info: Prompt Content:\n",
      "[{\"role\": \"system\", \"content\": \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {\"role\": \"user\", \"content\": \"Show me the nodes created in the last 7 days\"}]\n",
      "Info: Ollama Response:\n",
      "model='mistral:latest' created_at='2025-05-19T09:50:57.280455Z' done=True done_reason='stop' total_duration=2179508375 load_duration=12392208 prompt_eval_count=1056 prompt_eval_duration=1450008625 eval_count=44 eval_duration=710000625 message=Message(role='assistant', content=\" SELECT nodeid, nodename, clustername, instancetype, tags, created, snapshottime, platform\\n   FROM nodedata\\n   WHERE created >= now() - interval '7 days';\", images=None, tool_calls=None)\n",
      "LLM Response:  SELECT nodeid, nodename, clustername, instancetype, tags, created, snapshottime, platform\n",
      "   FROM nodedata\n",
      "   WHERE created >= now() - interval '7 days';\n",
      "Info: Output from LLM:  SELECT nodeid, nodename, clustername, instancetype, tags, created, snapshottime, platform\n",
      "   FROM nodedata\n",
      "   WHERE created >= now() - interval '7 days'; \n",
      "Extracted SQL: SELECT nodeid, nodename, clustername, instancetype, tags, created, snapshottime, platform\n",
      "   FROM nodedata\n",
      "   WHERE created >= now() - interval '7 days'\n",
      "SELECT nodeid, nodename, clustername, instancetype, tags, created, snapshottime, platform\n",
      "   FROM nodedata\n",
      "   WHERE created >= now() - interval '7 days'\n",
      "\n",
      "Query Result:\n",
      "                nodeid                                    nodename  \\\n",
      "0  i-01571c11d7b21e6e8   ip-10-79-149-5.us-west-2.compute.internal   \n",
      "1  i-0018c11bdf83bd9b2  ip-10-79-169-79.us-west-2.compute.internal   \n",
      "2  i-0089127172e2aa1cd  ip-10-79-88-134.us-west-2.compute.internal   \n",
      "3  i-0f6139ce192526357  ip-10-79-179-31.us-west-2.compute.internal   \n",
      "4  i-0abfd1ca834657fd5    ip-10-91-9-14.us-west-2.compute.internal   \n",
      "\n",
      "                 clustername   instancetype  \\\n",
      "0  sensei-eks04-prod-cluster  p4de.24xlarge   \n",
      "1  sensei-eks04-prod-cluster  p4de.24xlarge   \n",
      "2  sensei-eks04-prod-cluster  p4de.24xlarge   \n",
      "3  sensei-eks04-prod-cluster  p4de.24xlarge   \n",
      "4   colligo-laser01-prod-uw2  p4de.24xlarge   \n",
      "\n",
      "                                                tags             created  \\\n",
      "0  {'aws:eks:cluster-name': 'sensei-eks04-prod-cl... 2025-05-13 04:51:38   \n",
      "1  {'Adobe.Owner': 'Sensei Ops'; 'k8s.io/cluster-... 2025-05-12 17:50:51   \n",
      "2  {'k8s.io/cluster-autoscaler/enabled': 'true'; ... 2025-05-14 13:27:04   \n",
      "3  {'k8s.io/cluster-autoscaler/enabled': 'true'; ... 2025-05-14 06:00:18   \n",
      "4  {'aws:ec2launchtemplate:version': '1'; 'eks:no... 2025-05-13 18:11:32   \n",
      "\n",
      "                snapshottime platform  \n",
      "0 2025-05-13 23:29:18.231226    runai  \n",
      "1 2025-05-15 08:59:18.229355    runai  \n",
      "2 2025-05-15 08:59:18.229355    runai  \n",
      "3 2025-05-15 08:59:18.229355    runai  \n",
      "4 2025-05-15 09:01:53.516620    pluto  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test 3: What are the instance types used in each platform?\n",
      "\n",
      "Generated SQL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Prompt: [{'role': 'system', 'content': \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {'role': 'user', 'content': 'What are the instance types used in each platform?'}]\n",
      "Info: Ollama parameters:\n",
      "model=mistral:latest,\n",
      "options={},\n",
      "keep_alive=None\n",
      "Info: Prompt Content:\n",
      "[{\"role\": \"system\", \"content\": \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {\"role\": \"user\", \"content\": \"What are the instance types used in each platform?\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Ollama Response:\n",
      "model='mistral:latest' created_at='2025-05-19T09:51:00.270192Z' done=True done_reason='stop' total_duration=2763553042 load_duration=12418500 prompt_eval_count=1055 prompt_eval_duration=1453683667 eval_count=79 eval_duration=1291865333 message=Message(role='assistant', content=' To find the instance types used for each platform, you can use the following query:\\n\\n```sql\\nSELECT platform, instancetype\\nFROM nodedata\\nGROUP BY platform, instancetype;\\n```\\n\\nThis query groups the results by both platform and instancetype columns, allowing us to see the unique combinations of platforms and instance types.', images=None, tool_calls=None)\n",
      "LLM Response:  To find the instance types used for each platform, you can use the following query:\n",
      "\n",
      "```sql\n",
      "SELECT platform, instancetype\n",
      "FROM nodedata\n",
      "GROUP BY platform, instancetype;\n",
      "```\n",
      "\n",
      "This query groups the results by both platform and instancetype columns, allowing us to see the unique combinations of platforms and instance types.\n",
      "Info: Output from LLM:  To find the instance types used for each platform, you can use the following query:\n",
      "\n",
      "```sql\n",
      "SELECT platform, instancetype\n",
      "FROM nodedata\n",
      "GROUP BY platform, instancetype;\n",
      "```\n",
      "\n",
      "This query groups the results by both platform and instancetype columns, allowing us to see the unique combinations of platforms and instance types. \n",
      "Extracted SQL: SELECT platform, instancetype\n",
      "FROM nodedata\n",
      "GROUP BY platform, instancetype\n",
      "SELECT platform, instancetype\n",
      "FROM nodedata\n",
      "GROUP BY platform, instancetype\n",
      "\n",
      "Query Result:\n",
      "  platform   instancetype\n",
      "0    pluto   r6i.24xlarge\n",
      "1    pluto  p5en.48xlarge\n",
      "2    pluto    p5.48xlarge\n",
      "3    runai  p4de.24xlarge\n",
      "4    pluto   g6e.12xlarge\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test 4: What is the distribution of instance types?\n",
      "\n",
      "Generated SQL:\n",
      "SQL Prompt: [{'role': 'system', 'content': \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {'role': 'user', 'content': 'What is the distribution of instance types?'}]\n",
      "Info: Ollama parameters:\n",
      "model=mistral:latest,\n",
      "options={},\n",
      "keep_alive=None\n",
      "Info: Prompt Content:\n",
      "[{\"role\": \"system\", \"content\": \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {\"role\": \"user\", \"content\": \"What is the distribution of instance types?\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Ollama Response:\n",
      "model='mistral:latest' created_at='2025-05-19T09:51:01.913192Z' done=True done_reason='stop' total_duration=1466040167 load_duration=9882000 prompt_eval_count=1053 prompt_eval_duration=68248042 eval_count=79 eval_duration=1382498500 message=Message(role='assistant', content=\" To find the distribution of instance types in the 'nodedata' table, you can use the following query:\\n\\n```sql\\nSELECT instancetype, COUNT(instancetype) as count\\nFROM nodedata\\nGROUP BY instancetype;\\n```\\n\\nThis will return a result with each unique instance type and its respective count.\", images=None, tool_calls=None)\n",
      "LLM Response:  To find the distribution of instance types in the 'nodedata' table, you can use the following query:\n",
      "\n",
      "```sql\n",
      "SELECT instancetype, COUNT(instancetype) as count\n",
      "FROM nodedata\n",
      "GROUP BY instancetype;\n",
      "```\n",
      "\n",
      "This will return a result with each unique instance type and its respective count.\n",
      "Info: Output from LLM:  To find the distribution of instance types in the 'nodedata' table, you can use the following query:\n",
      "\n",
      "```sql\n",
      "SELECT instancetype, COUNT(instancetype) as count\n",
      "FROM nodedata\n",
      "GROUP BY instancetype;\n",
      "```\n",
      "\n",
      "This will return a result with each unique instance type and its respective count. \n",
      "Extracted SQL: SELECT instancetype, COUNT(instancetype) as count\n",
      "FROM nodedata\n",
      "GROUP BY instancetype\n",
      "SELECT instancetype, COUNT(instancetype) as count\n",
      "FROM nodedata\n",
      "GROUP BY instancetype\n",
      "\n",
      "Query Result:\n",
      "    instancetype  count\n",
      "0  p4de.24xlarge   1560\n",
      "1   r5dn.8xlarge      1\n",
      "2   c5d.24xlarge     31\n",
      "3   r6i.24xlarge      5\n",
      "4   p4d.24xlarge    265\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test 5: Show me the clusters that have more than 5 nodes, grouped by platform\n",
      "\n",
      "Generated SQL:\n",
      "SQL Prompt: [{'role': 'system', 'content': \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {'role': 'user', 'content': 'Show me the clusters that have more than 5 nodes, grouped by platform'}]\n",
      "Info: Ollama parameters:\n",
      "model=mistral:latest,\n",
      "options={},\n",
      "keep_alive=None\n",
      "Info: Prompt Content:\n",
      "[{\"role\": \"system\", \"content\": \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {\"role\": \"user\", \"content\": \"Show me the clusters that have more than 5 nodes, grouped by platform\"}]\n",
      "Info: Ollama Response:\n",
      "model='mistral:latest' created_at='2025-05-19T09:51:04.604779Z' done=True done_reason='stop' total_duration=2506604542 load_duration=9340167 prompt_eval_count=1061 prompt_eval_duration=1470386708 eval_count=63 eval_duration=1021627125 message=Message(role='assistant', content=\" SELECT clustername, COUNT(nodeid) as node_count, platform\\n   FROM nodedata\\n   WHERE snapshottime >= now() - interval '7 days'\\n   GROUP BY clustername, platform\\n   HAVING COUNT(nodeid) > 5;\", images=None, tool_calls=None)\n",
      "LLM Response:  SELECT clustername, COUNT(nodeid) as node_count, platform\n",
      "   FROM nodedata\n",
      "   WHERE snapshottime >= now() - interval '7 days'\n",
      "   GROUP BY clustername, platform\n",
      "   HAVING COUNT(nodeid) > 5;\n",
      "Info: Output from LLM:  SELECT clustername, COUNT(nodeid) as node_count, platform\n",
      "   FROM nodedata\n",
      "   WHERE snapshottime >= now() - interval '7 days'\n",
      "   GROUP BY clustername, platform\n",
      "   HAVING COUNT(nodeid) > 5; \n",
      "Extracted SQL: SELECT clustername, COUNT(nodeid) as node_count, platform\n",
      "   FROM nodedata\n",
      "   WHERE snapshottime >= now() - interval '7 days'\n",
      "   GROUP BY clustername, platform\n",
      "   HAVING COUNT(nodeid) > 5\n",
      "SELECT clustername, COUNT(nodeid) as node_count, platform\n",
      "   FROM nodedata\n",
      "   WHERE snapshottime >= now() - interval '7 days'\n",
      "   GROUP BY clustername, platform\n",
      "   HAVING COUNT(nodeid) > 5\n",
      "\n",
      "Query Result:\n",
      "                      clustername  node_count platform\n",
      "0       sensei-eks02-prod-cluster          96    runai\n",
      "1  colligo-laser-control-prod-uw2           7    pluto\n",
      "2        colligo-laser01-prod-uw2        1905    pluto\n",
      "3       sensei-eks04-prod-cluster         148    runai\n",
      "4       sensei-eks01-prod-cluster          10    runai\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test 6: What are all the nodes in the 'sensei-eks01-prod-cluster' cluster?\n",
      "\n",
      "Generated SQL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Prompt: [{'role': 'system', 'content': \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {'role': 'user', 'content': \"What are all the nodes in the 'sensei-eks01-prod-cluster' cluster?\"}]\n",
      "Info: Ollama parameters:\n",
      "model=mistral:latest,\n",
      "options={},\n",
      "keep_alive=None\n",
      "Info: Prompt Content:\n",
      "[{\"role\": \"system\", \"content\": \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {\"role\": \"user\", \"content\": \"What are all the nodes in the 'sensei-eks01-prod-cluster' cluster?\"}]\n",
      "Info: Ollama Response:\n",
      "model='mistral:latest' created_at='2025-05-19T09:51:06.807848Z' done=True done_reason='stop' total_duration=1971496333 load_duration=12199375 prompt_eval_count=1067 prompt_eval_duration=1455456250 eval_count=31 eval_duration=497268292 message=Message(role='assistant', content=\" SELECT nodeid, nodename FROM public.nodedata WHERE clustername = 'sensei-eks01-prod-cluster';\", images=None, tool_calls=None)\n",
      "LLM Response:  SELECT nodeid, nodename FROM public.nodedata WHERE clustername = 'sensei-eks01-prod-cluster';\n",
      "Info: Output from LLM:  SELECT nodeid, nodename FROM public.nodedata WHERE clustername = 'sensei-eks01-prod-cluster'; \n",
      "Extracted SQL: SELECT nodeid, nodename FROM public.nodedata WHERE clustername = 'sensei-eks01-prod-cluster'\n",
      "SELECT nodeid, nodename FROM public.nodedata WHERE clustername = 'sensei-eks01-prod-cluster'\n",
      "\n",
      "Query Result:\n",
      "                nodeid                                    nodename\n",
      "0  i-0ed1ca2cb5e64d8e9   ip-10-66-79-46.us-west-2.compute.internal\n",
      "1  i-083522813bf57b7d5  ip-10-66-73-195.us-west-2.compute.internal\n",
      "2  i-02177e7a59ec77230  ip-10-66-78-145.us-west-2.compute.internal\n",
      "3  i-0734be286729c0a12  ip-10-66-76-245.us-west-2.compute.internal\n",
      "4  i-019aa9f6e250dccea  ip-10-66-77-114.us-west-2.compute.internal\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test 7: Find all nodes that have 'compute' in their name\n",
      "\n",
      "Generated SQL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Prompt: [{'role': 'system', 'content': \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {'role': 'user', 'content': \"Find all nodes that have 'compute' in their name\"}]\n",
      "Info: Ollama parameters:\n",
      "model=mistral:latest,\n",
      "options={},\n",
      "keep_alive=None\n",
      "Info: Prompt Content:\n",
      "[{\"role\": \"system\", \"content\": \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {\"role\": \"user\", \"content\": \"Find all nodes that have 'compute' in their name\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Ollama Response:\n",
      "model='mistral:latest' created_at='2025-05-19T09:51:09.458676Z' done=True done_reason='stop' total_duration=2352128417 load_duration=12569667 prompt_eval_count=1056 prompt_eval_duration=1453036417 eval_count=54 eval_duration=881883708 message=Message(role='assistant', content=\" SELECT nodeid, nodename\\n   FROM nodedata\\n   WHERE clustername IS NOT NULL\\n     AND instancetype IS NOT NULL\\n     AND clustername LIKE '%compute%'\\n     AND nodename LIKE '%compute%';\", images=None, tool_calls=None)\n",
      "LLM Response:  SELECT nodeid, nodename\n",
      "   FROM nodedata\n",
      "   WHERE clustername IS NOT NULL\n",
      "     AND instancetype IS NOT NULL\n",
      "     AND clustername LIKE '%compute%'\n",
      "     AND nodename LIKE '%compute%';\n",
      "Info: Output from LLM:  SELECT nodeid, nodename\n",
      "   FROM nodedata\n",
      "   WHERE clustername IS NOT NULL\n",
      "     AND instancetype IS NOT NULL\n",
      "     AND clustername LIKE '%compute%'\n",
      "     AND nodename LIKE '%compute%'; \n",
      "Extracted SQL: SELECT nodeid, nodename\n",
      "   FROM nodedata\n",
      "   WHERE clustername IS NOT NULL\n",
      "     AND instancetype IS NOT NULL\n",
      "     AND clustername LIKE '%compute%'\n",
      "     AND nodename LIKE '%compute%'\n",
      "SELECT nodeid, nodename\n",
      "   FROM nodedata\n",
      "   WHERE clustername IS NOT NULL\n",
      "     AND instancetype IS NOT NULL\n",
      "     AND clustername LIKE '%compute%'\n",
      "     AND nodename LIKE '%compute%'\n",
      "\n",
      "Query Result:\n",
      "No results found\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test 8: Show the number of nodes created per day\n",
      "\n",
      "Generated SQL:\n",
      "SQL Prompt: [{'role': 'system', 'content': \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {'role': 'user', 'content': 'Show the number of nodes created per day'}]\n",
      "Info: Ollama parameters:\n",
      "model=mistral:latest,\n",
      "options={},\n",
      "keep_alive=None\n",
      "Info: Prompt Content:\n",
      "[{\"role\": \"system\", \"content\": \"You are a PostgreSQL expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Additional Context \\n\\n\\nYou are an AI assistant trained to generate SQL queries from user queries on table nodedata. The dataset contains instance metadata—including creation and snapshot timestamps—for nodes across clusters. Your goal is to accurately translate user questions into optimized SQL queries while handling potential issues such as:\\nPrimary Key:\\n• Ensure nodeid is treated as the primary key—each node is uniquely identified.\\n\\nDate Ranges:\\n• Interpret relative time windows on the created and snapshottime columns:\\n  - “Last 7 days” → WHERE snapshottime ≥ now() - interval '7 days'\\n  - “Created in Q1” → WHERE created BETWEEN '2025-01-01' AND '2025-03-31'\\n  - “Between date X and Y” → inclusive BETWEEN on created or snapshottime.\\n\\nHAVING Clause Rule:\\n• Do NOT use column aliases (e.g., node_count) in the HAVING clause. Instead, repeat the full aggregation function (e.g., HAVING COUNT(nodeid) > 5).\\n\\n\\nIn queries always refer to the table as 'nodedata' not as 'data.nodedata'.\\nGenerate syntactically correct PostgreSQL queries that can be directly executed.\\nAlways verify column existence and ensure that queries are efficient and correct.\\nIf a query involves calculations, wrap division operations with NULLIF(denominator, 0) to avoid errors.\\n\\n\\nThe following columns are in the nodedata table in the data database:\\n\\n|    | table_catalog   | table_schema   | table_name   | column_name   | data_type                   |\\n|---:|:----------------|:---------------|:-------------|:--------------|:----------------------------|\\n|  0 | data            | public         | nodedata     | nodeid        | text                        |\\n|  1 | data            | public         | nodedata     | nodename      | text                        |\\n|  2 | data            | public         | nodedata     | clustername   | text                        |\\n|  3 | data            | public         | nodedata     | instancetype  | text                        |\\n|  4 | data            | public         | nodedata     | tags          | text                        |\\n|  5 | data            | public         | nodedata     | created       | timestamp without time zone |\\n|  6 | data            | public         | nodedata     | snapshottime  | timestamp without time zone |\\n|  7 | data            | public         | nodedata     | platform      | text                        |\\n\\n\\nThe table name is 'nodedata' and it is present in the 'public' schema of the database 'data'.\\nNote the schema of the table nodedatafor which you will be generating queries:\\n    nodeid        TEXT,    -- Unique identifier for the node (PRIMARY KEY)\\n    nodename      TEXT,    -- Name of the node\\n    clustername   TEXT,    -- Name of the Kubernetes or compute cluster\\n    instancetype  TEXT,    -- EC2 instance type or VM size (e.g., c5.18xlarge, p4d.24xlarge)\\n    tags          TEXT,    -- JSON-style string of metadata tags (key/value pairs)\\n    created       TIMESTAMP WITHOUT TIME ZONE, -- When the node was first created\\n    snapshottime  TIMESTAMP WITHOUT TIME ZONE, -- When this snapshot of node metadata was taken\\n    platform      TEXT     -- Runtime platform or orchestration tool (e.g., runai, kubernetes)\\n)\\n\\n\\n===Response Guidelines \\n1. If the provided context is sufficient, please generate a valid SQL query without any explanations for the question. \\n2. If the provided context is almost sufficient but requires knowledge of a specific string in a particular column, please generate an intermediate SQL query to find the distinct strings in that column. Prepend the query with a comment saying intermediate_sql \\n3. If the provided context is insufficient, please explain why it can't be generated. \\n4. Please use the most relevant table(s). \\n5. If the question has been asked and answered before, please repeat the answer exactly as it was given before. \\n6. Ensure that the output SQL is PostgreSQL-compliant and executable, and free of syntax errors. \\n\"}, {\"role\": \"user\", \"content\": \"Show the number of nodes created per day\"}]\n",
      "Info: Ollama Response:\n",
      "model='mistral:latest' created_at='2025-05-19T09:51:12.041004Z' done=True done_reason='stop' total_duration=2385945541 load_duration=9465166 prompt_eval_count=1053 prompt_eval_duration=1464829917 eval_count=56 eval_duration=905184750 message=Message(role='assistant', content=\" SELECT DATE(created) AS creation_date, COUNT(nodeid) AS node_count\\nFROM nodedata\\nWHERE created >= now() - interval '7 days'\\nGROUP BY creation_date\\nORDER BY creation_date DESC;\", images=None, tool_calls=None)\n",
      "LLM Response:  SELECT DATE(created) AS creation_date, COUNT(nodeid) AS node_count\n",
      "FROM nodedata\n",
      "WHERE created >= now() - interval '7 days'\n",
      "GROUP BY creation_date\n",
      "ORDER BY creation_date DESC;\n",
      "Info: Output from LLM:  SELECT DATE(created) AS creation_date, COUNT(nodeid) AS node_count\n",
      "FROM nodedata\n",
      "WHERE created >= now() - interval '7 days'\n",
      "GROUP BY creation_date\n",
      "ORDER BY creation_date DESC; \n",
      "Extracted SQL: SELECT DATE(created) AS creation_date, COUNT(nodeid) AS node_count\n",
      "FROM nodedata\n",
      "WHERE created >= now() - interval '7 days'\n",
      "GROUP BY creation_date\n",
      "ORDER BY creation_date DESC\n",
      "SELECT DATE(created) AS creation_date, COUNT(nodeid) AS node_count\n",
      "FROM nodedata\n",
      "WHERE created >= now() - interval '7 days'\n",
      "GROUP BY creation_date\n",
      "ORDER BY creation_date DESC\n",
      "\n",
      "Query Result:\n",
      "  creation_date  node_count\n",
      "0    2025-05-15           4\n",
      "1    2025-05-14          11\n",
      "2    2025-05-13          34\n",
      "3    2025-05-12          10\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data Validation Tests:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Platform Distribution:\n",
      "  platform  node_count\n",
      "0    pluto        2202\n",
      "1    runai         658\n",
      "\n",
      "Date Range and Total Nodes:\n",
      "        earliest_date         latest_date  total_nodes\n",
      "0 2023-12-07 09:48:17 2025-05-15 06:59:11         2860\n",
      "\n",
      "Instance Type Distribution:\n",
      "    instancetype  count              percentage\n",
      "0  p4de.24xlarge   1560     54.5454545454545455\n",
      "1    p5.48xlarge    849     29.6853146853146853\n",
      "2   p4d.24xlarge    265      9.2657342657342657\n",
      "3  p5en.48xlarge     80      2.7972027972027972\n",
      "4    c5.18xlarge     53      1.8531468531468531\n",
      "5   c5d.24xlarge     31      1.0839160839160839\n",
      "6   c6i.24xlarge     12  0.41958041958041958042\n",
      "7   r6i.24xlarge      5  0.17482517482517482517\n",
      "8   g6e.12xlarge      4  0.13986013986013986014\n",
      "9   r5dn.8xlarge      1  0.03496503496503496503\n"
     ]
    }
   ],
   "source": [
    "# Test questions to verify different aspects of the model\n",
    "test_questions = [\n",
    "    # Basic counting and grouping\n",
    "    \"How many nodes are there per cluster?\",\n",
    "    \n",
    "    # Time-based analysis\n",
    "    \"Show me the nodes created in the last 7 days\",\n",
    "    \n",
    "    # Multiple conditions\n",
    "    \"What are the instance types used in each platform?\",\n",
    "    \n",
    "    # Aggregations\n",
    "    \"What is the distribution of instance types?\",\n",
    "    \n",
    "    # Complex combinations\n",
    "    \"Show me the clusters that have more than 5 nodes, grouped by platform\",\n",
    "    \n",
    "    # Specific lookups\n",
    "    \"What are all the nodes in the 'sensei-eks01-prod-cluster' cluster?\",\n",
    "    \n",
    "    # Pattern matching\n",
    "    \"Find all nodes that have 'compute' in their name\",\n",
    "    \n",
    "    # Time series\n",
    "    \"Show the number of nodes created per day\"\n",
    "]\n",
    "\n",
    "# Function to test and display results\n",
    "def test_vanna_model(vn, questions):\n",
    "    print(\"Testing Vanna Model with Various Questions\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\nTest {i}: {question}\")\n",
    "        print(\"\\nGenerated SQL:\")\n",
    "        try:\n",
    "            sql = vn.generate_sql(question)\n",
    "            print(sql)\n",
    "            \n",
    "            print(\"\\nQuery Result:\")\n",
    "            result = vn.run_sql(sql)\n",
    "            print(result.head() if not result.empty else \"No results found\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Run the tests\n",
    "test_vanna_model(vn, test_questions)\n",
    "\n",
    "# Additional specific test for data validation\n",
    "print(\"\\nData Validation Tests:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Test 1: Check unique platforms\n",
    "sql_platforms = \"SELECT DISTINCT platform, COUNT(*) as node_count FROM public.nodedata GROUP BY platform ORDER BY node_count DESC;\"\n",
    "print(\"\\nPlatform Distribution:\")\n",
    "print(vn.run_sql(sql_platforms))\n",
    "\n",
    "# Test 2: Check date range\n",
    "sql_dates = \"\"\"\n",
    "SELECT \n",
    "    MIN(created) as earliest_date,\n",
    "    MAX(created) as latest_date,\n",
    "    COUNT(*) as total_nodes\n",
    "FROM public.nodedata;\n",
    "\"\"\"\n",
    "print(\"\\nDate Range and Total Nodes:\")\n",
    "print(vn.run_sql(sql_dates))\n",
    "\n",
    "# Test 3: Instance type summary\n",
    "sql_instances = \"\"\"\n",
    "SELECT \n",
    "    instancetype,\n",
    "    COUNT(*) as count,\n",
    "    COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () as percentage\n",
    "FROM public.nodedata\n",
    "GROUP BY instancetype\n",
    "ORDER BY count DESC;\n",
    "\"\"\"\n",
    "print(\"\\nInstance Type Distribution:\")\n",
    "print(vn.run_sql(sql_instances))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
